---
- name: Initial configuration of Ceph
  when: "inventory_hostname == groups[pve_ceph_mon_group][0]"
  block:
    - name: Create initial Ceph config
      ansible.builtin.command: "pveceph init --network {{ pve_ceph_network }}  \
               {% if pve_ceph_cluster_network is defined %} \
                  --cluster-network {{ pve_ceph_cluster_network }}
               {% endif %}"
      args:
        creates: /etc/ceph/ceph.conf

    - name: Create initial Ceph monitor
      ansible.builtin.command: 'pveceph mon create'
      args:
        creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
      register: _ceph_initial_mon

    - name: Fail if initial monitor creation failed
      ansible.builtin.fail:
        msg: 'Ceph intial monitor creation failed.'
      when: _ceph_initial_mon is failed

- name: Create additional Ceph monitors
  ansible.builtin.command: 'pveceph mon create'
  args:
    creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
  when:
    - "inventory_hostname != groups[pve_ceph_mon_group][0]"
    - "inventory_hostname in groups[pve_ceph_mon_group]"

- name: Create additional Ceph managers
  ansible.builtin.command: 'pveceph mgr create'
  args:
    creates: '/var/lib/ceph/mgr/ceph-{{ ansible_hostname }}/'
  when: "inventory_hostname in groups[pve_ceph_mgr_group]"

- name: Clear facts
  ansible.builtin.meta: clear_facts
  when: pve_ceph_install and pve_ceph_osds_enabled
  tags: ceph_install

- name: Gather facts about available disks
  # These tasks gathers facts about the available disks on the Proxmox nodes.
  # It automatically detects SATA and NVMe devices, as these can change names
  # during reboots. It ensures that the boot device is not included in the OSDs.
  block:
    - name: Get non-efi boot drive
      ansible.builtin.shell:
        lsblk -no PKNAME "$(findmnt -no SOURCE /boot)"
      register: _non_efi_boot_device
      no_log: true
      ignore_errors: true

    - name: Get efi boot drive
      when: _non_efi_boot_device is failed
      ansible.builtin.shell:
        lsblk -no PKNAME "$(findmnt -no SOURCE /boot/efi)"
      register: _efi_boot_device
      no_log: true

    - name: Set non-efi boot device
      ansible.builtin.set_fact:
        _non_efi_boot_device: "{{ _non_efi_boot_device.stdout | default('') | trim }}"
      no_log: true

    - name: Set efi boot device
      ansible.builtin.set_fact:
        _efi_boot_device: "{{ _efi_boot_device.stdout | default('') | trim }}"

    - name: Generate OSDs for SATA devices
      ansible.builtin.set_fact:
        _sata_osds: "{{ _sata_osds + [{'device': '/dev/' + item.key, 'crush.device.class': 'hdd'}] }}"
      no_log: true
      with_dict: "{{ ansible_devices }}"
      when: (item.value.host is defined and
            (item.value.host.startswith('SATA controller:')) and
            (not item.key.startswith('loop'))  and
            (not item.key.startswith('dm'))  and
            (item.key not in _non_efi_boot_device) and
            (item.key not in _efi_boot_device))
      loop_control:
        label: "{{ item.key }}"

    - name: Generate OSDs for NVMe devices
      ansible.builtin.set_fact:
        _nvme_osds: "{{ _nvme_osds + [{'device': '/dev/' + item.key, 'crush.device.class': 'nvme'}] }}"
      no_log: true
      with_dict: "{{ ansible_devices }}"
      when: (item.value.host is defined and
            (item.value.host.startswith('Non-Volatile memory controller:')) and
            (not item.key.startswith('loop'))  and
            (not item.key.startswith('dm'))  and
            (item.key not in _non_efi_boot_device) and
            (item.key not in _efi_boot_device))
      loop_control:
        label: "{{ item.key }}"

    - name: Combine SATA and NVMe OSDs
      ansible.builtin.set_fact:
        _ceph_osds: "{{ _sata_osds + _nvme_osds }}"
      no_log: true

- name: Destroy Ceph LVM on first run
  # This task is used to zap the LVM disks on first run after a reinstallation.
  # It ensures that the disks are clean before creating new OSDs.
  # This task is run only once to avoid unnecessary destruction of data.
  # If the disks are not destroyed, the OSD creation will fail.
  ansible.builtin.command: ceph-volume lvm zap {{ item.device }} --destroy
  when: pve_ceph_install and pve_ceph_osds_enabled and _ceph_osds | length > 0
  loop: '{{ _ceph_osds }}'
  run_once: true
  register: _ceph_lvm_zap
  changed_when: _ceph_lvm_zap is changed
  args:
    warn: false
  ignore_errors: true

- name: Create Ceph OSDs
  block:
    - name: Query for existing Ceph volumes
      pve_ceph_volume:
      check_mode: no
      register: _ceph_volume_data

    - name: Generate a list of active OSDs
      ansible.builtin.set_fact:
        _existing_ceph_osds: "{{ _ceph_volume_data.stdout | from_json | json_query('*[].devices[]') | default([]) }}"

    - name: Generate list of unprovisioned OSDs
      ansible.builtin.set_fact:
        _ceph_osds_diff: "{{ _ceph_osds_diff | default([]) + [item] }}"
      loop: "{{ _ceph_osds }}"
      when: item.device not in _existing_ceph_osds

    - name: Create Ceph OSDs
      ansible.builtin.command: >-
        pveceph osd create {{ item.device }}
        {% if "crush.device.class" in item %}--crush-device-class {{ item["crush.device.class"] }}{% endif %}
        {% if "encrypted" in item and item["encrypted"] | bool %}--encrypted 1{% endif %}
        {% if "block.db" in item %}--db_dev {{ item["block.db"] }}{% endif %}
        {% if "block.wal" in item %}--wal_dev {{ item["block.wal"] }}{% endif %}
      loop: '{{ _ceph_osds_diff | default([]) }}'
      when: _ceph_osds_diff | length > 0
      register: _ceph_osd_create
